

\let\textcircled=\pgftextcircled\chapter{Future Work}\label{chap:future_work}

% Reward Functions
\textbf{Curriculum}

It is easy to imagine to the limitless possible combinations of rewards available, and as some of the desired behaviour was observed in the simulation, it is clear that the reward function was on the right lines. However, as explained earlier, due to the complex nature of the challenge at hand, a curriculum was used to split the reward function into smaller, more manageable stages. This proved beneficial but there was room for dissecting of the task into even smaller stages. This would have allowed for a more gradual learning process and hopefully given the agent a better chance at retaining what it had learnt in the previous stage. It would also have been good to try multiple different curriculum's to see if the agent could learn the task in a different order.


\textbf{Evaluation of Performance}

It would be advantageous to setup a comprehensive evaluation process that could be conducted in between each training run. This would provide clear incite into the runs success and help provide direction for the next training run. While training a complex system it is easy to get distracted in nuances of the training process as there are so many potential different factors effecting the outcome. 

\textbf{Simulation}

The simulation (training environment), albeit not perfect, was a good representation of the real world, and was a good starting point for the project. In future it would be good to create a more accurate simulation, representing the real world as closely as possible. This would allow for easier integration into a physical system, should the training reach a point where it is ready to be tested in the real world. To create a better simulation, the first aspect to be addressed would be the water model. Remove any water system and create a particle simulation from scratch, this would allow for proper mechanics and realistic water behaviour.

Once the agent has learnt the simpler task of the kiteboat controls, it would be good to start adding in adverse weather conditions. This would be in the form of non laminar wind, i.e. gusty shifty wind, and varying waves. These conditions must be added to the model if the agent is ever going to progress to the point of becoming reliable in the real world, as these are the conditions that the agent will be faced with. Once a reward function has been found that allows the agent to learn the task, it is at this point where the conditions should be added and the agent retrained from scratch; it would be interesting to observe weather this same reward function is still able to learn the task in the new conditions, or if a new reward function is required.

Another part of improving the simulation (the water system), would be to add the ability for water relaunching. LEI kites have the ability to relaunch from the water, and this is a key reason for their popularity in the kiteboarding community. Adding the mechanics for water relaunching into the simulations and training the agent to perform this task would be a good next step, improving the reliability and robustness of the agent. One of the main concerns for the viability autonomously controlled kites is what happens if the kite crashes into the water. It is easy to imagine many dangerous and difficult situations that could arise, and so it os important to tray and mitigate these risks as early as possible to improve the change of success in the real world.