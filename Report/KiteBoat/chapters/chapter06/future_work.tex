

\let\textcircled=\pgftextcircled\chapter{Future Work}\label{chap:future_work}

% Reward Functions
\subsection*{Simulation Environment}

The simulation (training environment), albeit not perfect, was a good representation of the real world, and was a good starting point for the project. In future it would be good to create a more accurate simulation, representing the real world as closely as possible. This would allow for easier integration into a physical system, should the training reach a point where it is ready to be tested in the real world. To create a better simulation, the first aspect to be addressed would be the water model. Remove any water system and create a particle simulation from scratch, this would allow for proper mechanics and realistic water behaviour.

Once the agent has learnt the simpler task of the kiteboat controls, it would be good to start adding in adverse weather conditions. This would be in the form of non laminar wind, i.e. gusty shifty wind, and varying waves. These conditions must be added to the model if the agent is ever going to progress to the point of becoming reliable in the real world, as these are the conditions that the agent will be faced with. Once a reward function has been found that allows the agent to learn the task, it is at this point where the conditions should be added and the agent retrained from scratch; it would be interesting to observe weather this same reward function is still able to learn the task in the new conditions, or if a new reward function is required.

Another part of improving the simulation (the water system), would be to add the ability for water relaunching. LEI kites have the ability to relaunch from the water, and this is a key reason for their popularity in the kiteboarding community. Adding the mechanics for water relaunching into the simulations and training the agent to perform this task would be a good next step, improving the reliability and robustness of the agent. One of the main concerns for the viability autonomously controlled kites is what happens if the kite crashes into the water. It is easy to imagine many dangerous and difficult situations that could arise, and so it is important to try and mitigate these risks as early as possible to improve the change of success in the real world. 


% The addition of a water relaunch mechanism would also change the way the rewards were calculated, if the kite crashed into the water the episode would no longer be over, and so the reward function would need to be adjusted to reflect this.
\subsection*{Curriculum}

It is easy to imagine the limitless possible combinations of rewards available, and as some of the desired behaviour was observed in the simulation, it is clear that the reward function was on the right lines. However, as explained earlier, due to the complex nature of the challenge at hand, a curriculum was used to split the reward function into smaller, more manageable stages, which proved beneficial but there was room for dissecting the task into even smaller stages. This would have allowed for a more gradual learning process and hopefully given the agent a better chance at retaining what it had learnt in the previous stage. It would also have been good to try multiple different curriculum's to see if the agent could learn the task in a different order.

\subsection*{Agent Architecture}
It is worth noting that the pursuit of a single agent that would be capable of full autonomous control may not be the best approach. It may be more beneficial to split the challenge and assign an specific agent to each task, for instance one agent for the kite control working with a hard coded steering algorithm. Another agent could be employed to direct the kite agent into different `modes', i.e to the left/right, close haul/bare away, based on the heading of the boat. This would massively simplify the task for each agent allowing for a much simpler reward function hopefully producing more reliable and successful results for the specific tasks. If the rudder control is removed from the agent it may be beneficial to use a control algorithm that is more suited to this task, such as a PID controller. Together these agents would be able to control the vessel, however this still has its own challenges, such as how to coordinate the agents and how to train them together for the same application. 


% \subsection*{Evaluation of Performance}
% It would be advantageous to setup a comprehensive evaluation process that could be conducted in between each training run. This would provide clear incite into the runs success and help provide direction for the next training run. While training a complex system it is easy to get distracted in nuances of the training process as there are so many potential different factors effecting the outcome. 

\section{Conclusion}
This research successfully demonstrates the feasibility and potential of using reinforcement learning for autonomous control of kite-powered vessels. The project developed a simulation environment that emulated real world conditions and integrated a kite propulsion system with a boat model. Utilising the MLAgents toolkit and Proximal Policy Optimisation (PPO) networks, the RL agent exhibited proficiency in managing the dual aspects of the boat's rudder and the kite's flight mechanics.

However the agent did not manage full autonomous control of the kiteboat and failed to produce the emergent behaviour of maneuvers such as tacking. This shortfall was primarily due to the complexity of the task, the limitations of the simulation environment and the tuning of the RL configuration. Future endeavours should concentrate on improving the simulation environment and refining the model in order to produce more reliable results under more complex and varying conditions and scenarios. The project successfully demonstrated the ability of an RL agent to learn multiple complex tasks, and provided a solid foundation for future research into the field of autonomous kite-powered vessels. There is a compelling argument to move away from fuel powered vessels and towards more sustainable and environmentally friendly alternatives, and this research has shown that RL could be a viable and promising solution to this problem.

% This study has effectively demonstrated the potential and feasibility of employing reinforcement learning (RL) for the autonomous control of kite-powered vessels, a pivotal step towards eco-friendly maritime technology. The project involved the development of a sophisticated simulation environment that emulated real-world maritime conditions, incorporating a synergistic model of kite propulsion and boat dynamics. utilising the MLAgents toolkit and Proximal Policy Optimisation (PPO) networks, the RL agent exhibited proficiency in managing the dual aspects of the boat's rudder and the kite's flight mechanics.

% However, it is important to note that the agent did not achieve complete autonomous control, particularly in executing complex maneuvers like tacking. This shortfall primarily stemmed from the inherent complexity of the task, coupled with certain limitations in the simulation environment and the specific configuration of the RL algorithm. These challenges, while highlighting areas for improvement, do not detract from the significant strides made in this research domain.

% Future endeavors should concentrate on refining the simulation environment, enhancing the RL model for more consistent performance under diverse and challenging conditions. The ability of the RL agent to learn and execute multiple complex tasks lays a robust foundation for continued exploration in the realm of autonomous kite-powered maritime vessels. The urgency to transition from traditional fuel-powered vessels to more sustainable alternatives has never been more pressing. This research substantiates the viability of RL as a promising solution in this paradigm shift towards sustainable maritime travel.