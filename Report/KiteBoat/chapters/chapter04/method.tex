% Author: Joshua Carey

%
\let\textcircled=\pgftextcircled\chapter{Methodology}
% \label{chap:aims_and_opjectives}

% \initial{T}his section will investigate the approach taken to create an autonomous kite-powered vessel.
% - We need a physics engine
% -we need to model a boat so that it behaves with realistic movements
% - need to be able to run machine learning in the physics simulations

% \initial{T}he future of autonomous navigation in maritime settings is not limited to large vessels plying the world's oceans; smaller, nimble crafts, harnessing natural forces such as wind, present their own set of challenges and opportunities. Imagine a vessel, propelled not just by the currents below, but by the gusts above, using a kite to harness the power of the wind. This not only promises sustainable navigation but also offers a glimpse into the intricate dance between machine intelligence, physics, and nature's unpredictability. This project dives deep into simulating such a systemâ€” a kite-powered boat that autonomously navigates its environment. This methodology section elucidates the steps taken to bridge the gap between vision and virtual reality.
\initial{T}his project will utilise the Unity game engine as the training environment for RL\@.


\section{MLAgents}
MLAgents is an open-source project that allows games and simulations to serve as the environment for training intelligent agents. At its core MLAgents utilizes RL, although it also supports other methods such as imitation learning.

\subsection{Python Implementation}
The neural network used in the reinforcement learning is implemented in Python. MLAgents toolkit is a Python Library that acts as an interface between the environment (gym), in this case Unity, and PyTorch [CITE]. PyTorch is an open-source machine learning library based on the Torch library, known for its flexibility, ease of use, and native support for GPU acceleration, which is essential for the computation-heavy processes involved in training neural networks. Torch is a Python-based scientific computing package that provides prebuilt components for machine learning and deep learning, as well as a wide range of mathematical functions. MLAgents uses a python API to communicate with the Unity environment frame by frame. This stepping process allows for the synchronous collection of observations, executions of actions and retrieval of rewards. The neural network used in this project is a Proximal Policy Optimization (PPO) network, and so will utilise the actor-critic method as discussed in section$~$\ref{sec:ppo_background}.

As discussed in section$~$\ref{RL_background}, RL is an approach to learning where an agent learns to make decisions by interacting with its environment. The fundamental components of this interaction with the environment are observations, actions and rewards.
\begin{itemize}
    \item Observations (State): These are the pieces of information that the agent receives from the environment at each step or frame. In Unity, observations are collected through sensors or manually coded to be extracted from the game objects. They are typically fed into the neural network as a vector of floating-point numbers, representing the current state of the environment.
    \item Actions: Based on the observations, the agent takes actions which are the outputs of the neural network. These actions can be discrete (e.g., turn left, turn right) or continuous (e.g., change angle by a certain degree). The neural network's output layer is designed accordingly to provide the appropriate action space for the agent. (Configured as part of the behavioral parameters in Unity)
    \item Rewards: After taking an action, the agent receives a reward signal, which is a numerical value indicating how well the action contributed to achieving its goal. This reward is used to adjust the neural network's weights, with the aim of maximizing the total accumulated reward.
\end{itemize}
A full breakdown of the actions, observations, rewards, and how the agent script configures these for this project can be found in section$~$\ref{sec:RL_Implementation}.

A sequence diagram of the interaction between the Unity environment and the Python neural network can be seen in figure$~$\ref{MLAgents_Seq_Diagram}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/MLAgents_Seq_Diagram.png}
    \caption{MLAgents Sequence Diagram}\label{MLAgents_Seq_Diagram}
\end{figure}

% sequenceDiagram
%     participant Unity as Unity (Gym Environment)
%     participant MLAgents as ML-Agents Toolkit
%     participant PyTorch as PyTorch (Neural Net)

%     Unity->>MLAgents: Observations (State)
%     MLAgents->>PyTorch: Processed Observations
%     PyTorch-->>MLAgents: Action Predictions
%     MLAgents-->>Unity: Actions
%     Unity->>MLAgents: Rewards / New State
%     MLAgents->>PyTorch: Process Rewards
%     PyTorch->>PyTorch: Update Weights


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/unity_mlagents.png}
    \caption{MLAgents Integration}\label{MLAgents_Integration}
\end{figure}



The technical instructions to setup MLAgents in Python and Unity can be found at 
\url{https://github.com/lipj01/AI-Kite-Control}

\section{The Environment}

The first step to this process is to create the environment, which the agent will use to train. In this case that will need to look something like a sailing game; it will have some form of water, a boat, a kite and a course. 
For any machine learning endeavor, especially one with such intricate physical dynamics, the choice of simulation environment is paramount. Not only does it provide the playground for our AI agent to learn and make mistakes safely, but it also serves as a litmus test for the robustness and realism of the designed model.

Given the myriad of choices available, the Unity game engine emerged as the most suitable platform. Beyond its reputation in gaming, 
The inherent support for mesh bodies, colliders, and a variety of joints made it an attractive option for simulating the kite-boat system, which comprised a complex dance of forces, and counterforces.
Unity is recognized for its potent physics engine, however for this project the use of Unity's physics engine will be kept to a minimum. Unity will be used primarily for its visual capabilities and the ability to run machine learning simulations. 

Central to our simulation is the depiction of water, the medium in which our boat will navigate. Here, the Unity HDRP Water System 16.0.3 [cite] came to the rescue. Bundled with Unity 2023.2.0b9 [cite], this water system provides a realistic representation of water with its undulating waves, refractions, and reflections. 
The alternative option to using Unity's water system was to model an entire particle fluid simulation, this would have had its advantages, however it would have been a lot more computationally expensive and would have taken a lot longer to implement. As this project was primarily focused on creating a RL algorithm for controlling a kiteboat it was decided that the Unity water system would be sufficient for this project.  

The boat part of the kiteboat had two main component scripts, the buoyancy and the rudder, allowing the boat to float and be steered. The implementation of Buoyancy and the rudder are discussed in more detail in section$~$\ref{sec:Boat}. The explanation of the kite model can be found in section$~$\ref{sec:Kite}.


\subsection{Boat Model}\label{sec:Boat}

\textbf{Boat Assumptions}
\begin{itemize}
    \item The Archimedes force is uniform across all submerged sections of the boat.
    \item The rudder forces of lift and drag could be aproximated to a torque applied about the rear of the boat.
    \item Once the boat is moving at a speed greater than 0.25m/s the lift and drag forces of the keel are equal to the downwind component of the kite's resultant force- essentially providing a non'slip condition.
\end{itemize}
Buoyancy, the force that allows ships to float, was the first physical property to be addressed. Rooted in Archimedes' Principle, it dictates that the buoyant force exerted on a submerged body is equivalent to the weight of the fluid displaced by that body. In our Unity environment, the boat's hull, represented as a `mesh' with an associated `mesh collider', was divided into many small triangles or Voxels. These Voxels became the fundamental units for calculating buoyancy, allowing for a granular and realistic representation of the boat's interaction with water. This was achieved by first calculating the total Archimedes force (AF) of the entire boat using equation$~$\ref{archimedes}, followed by a local AF at each Voxel. The water level, y component, was then computed at each voxel's (x,z) coordinates to determine if it was above or below the surface. If below the surface the component of the AF was applied vertically at each voxel. This implementation can be viewed in the buoy.cs script in the project REF IN APENDIX.

\begin{equation}
    F_B = \rho_{w}gV
    \label{archimedes}
\end{equation}

While buoyancy ensures our boat doesn't sink, it's the rudder that grants it direction. The Rudder.cs script handles the implementation of the rudder and the keel. The equation for the torque applied about the rear of the boat is shown in equation$~$\ref{rudder_torque}. 

% the torque equation
% float turnForce = angle * boatForwardSpeed * rotationScale;
% boatRb.AddTorque(transform.up * turnForce);
\begin{equation}
    \tau = \alpha v R
    \label{rudder_torque}
\end{equation}

where $\tau$ is the torque, $\alpha$ is the angle of the rudder, $v$ is the speed of the boat and $R$ is the rotation scale.

\subsection{Kite Model}\label{sec:Kite}
Capturing the intricate movements of a kite as it fly's through the air involves a complex balance between theoretical aerodynamics and the unpredictability of real-world conditions. In this model several assumptions were made to streamline the complexity into a more manageable form and are shown below.

\textbf{Kite Assumptions}
\begin{itemize}
    \item The kite is modeled as a symmetrical aerofoil, with constant lift and drag coefficients.
    \item Constant wind angle and laminar flow over the entire kite.
    \item The kite is always in the air, for the initial model the case where the kite crashes and requires relaunching was not considered. This would require adding buoyancy to the kite.
\end{itemize}

This sections outlines a kite model that, while simplified, serves as an effective tool for designing and testing the RL algorithm. The model is geared towards a realistic representation of the kites behavior and its response to control inputs. The kite chosen for this project was a Leading Edge Inflatable (LEI) kite, which is the most popular and mass produced recreational style of kites that exists. These kites connect to a control bar via 4 dyneema kite lines. Two center power lines take the load of the kite, while the outside two are responsible for steering, as shown in figure$~$\ref{kite_diagram}.

% insert kite diagram
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/kitediagram.jpg}
    \caption{Kite Diagram}\label{kite_diagram}
\end{figure}

The kite model was implemented using the kite.cs script. The kite was modeled as a symmetrical aerofoil, and implemented as a rigid body with constant lift and drag coefficients. The lift and drag forces were calculated using the equations shown in equation$~$\ref{lift} and equation$~$\ref{drag}.
\begin{equation}
    F_L = \rho C_L A \frac{v^2}{2} 
    \label{lift}
\end{equation}fixed at certain lengths, designed to permit movement across all rotational axis without any
\begin{equation}
    F_D = \rho v^2 C_D A \frac{v^2}{2} 
    \label{drag}
\end{equation}

where $F_L$ is the lift force, $F_D$ is the drag force, $\rho$ is the density of air, $C_L$ is the lift coefficient, $C_D$ is the drag coefficient, $A$ is the area of the kite and $v$ is the velocity of the kite relative to the wind.

In order to replicate the kite's mechanics and bar configuration, the model uses 4 configurable joints. These joints were fixed at certain lengths from the deck of the boat, designed to permit movement across all rotational axis without any `bounce' effect. This design enables the kite to descend or `fall' in conditions of low wind, mirroring real-world behavior where the kite may lose altitude but can be maneuvered back into position. To simulate the bar being pulled in, the lift and drag coefficients were increased, this has the effect of increasing the angle of attack of the kite.


\subsection{Collision Detection}
The Gilbert-Johnson-Keerthi (GJK)$~$\cite{gilbert88gjk} algorithm is a sophisticated method for collision detection between convex shapes. This algorithm was the approach taken to detect weather the kiteboat had reached the waypoint during training, and later, to work out if it had rounded the marks of the racecourse. The implementation of the GJK algorithm can be found in the GJK.cs script in the project REF IN APENDIX.
The GJK algorithm operates by iterative refinement of a simplex, which is a set of points that can define a line segment, triangle, or tetrahedron. The algorithm progresses by assessing whether the simplex contains the origin, which would imply an overlap between the two shapes. The initial direction d is determined by the normalized vector from mf1Pos to mf2Pos, constrained in the x-z plane by nullifying the y component, meaning the algorithm is only concerned with the horizontal plane.  

Within the GJK method, the Support function plays a pivotal role, calculating the Minkowski difference between the two shapes in a specified direction. This is achieved by finding the farthest points along that direction on both shapes and then subtracting them to obtain a single point in the Minkowski space.

The HandleSimplex function is a recursive strategy that adjusts the simplex and direction d based on whether the current simplex is a line or triangle. For a line, the LineCase function is invoked, and for a triangle, the TriangleCase is employed. These functions adjust the simplex and direction of search to move closer to the origin, if it is not already contained within the simplex.

\subsection{Course Generation}

% \subsection{Mark Roundings}

\section{Controls}
In order to ensure the AI would be able to learn to sail the kiteboat a playable game version was created. As discussed above the kite was modeled using 4 configurable joints to replicate the line system. There are several ways of configuring these so that a simple control input will result in the desired movement of the kite. 



\section{RL Implementation}\label{sec:RL_Implementation}
\subsection{The Agent Script}
The kiteboat agent sets up the scene for learning and in order for Unity to correctly process the script it must have the following 4 functions implemented:
\begin{itemize}
    \item OnEpisodeBegin()
    \item CollectObservations(VectorSensor sensor)
    \item OnActionReceived(ActionBuffers actions)
    \item Heuristic(in ActionBuffers actionsOut)
\end{itemize}

These are the minimum requirements for the agent setup, however there are several other functions that can be implemented to further customize the agent, handle errors and provide additional information. The agent starts by taking in the Kite and the Boat rigid bodies, as well as the rudder and kite scripts. A new gjkCollisionDetection is also initialised. The script starts with the OnEpisodeBegin method, which in turn starts by ensuring the training environment has been reset. The reset method sets the velocities and angular velocities of all rigid bodies in the scene to 0 and returns the kiteboat to a starting position. The remaining methods will be discussed in more detail below.
\subsection{Observations}
CollectObservations provides the network will all the information about the State of the environment and so aim to provide all the required information for the agent to make an informed decision. This means fully describing the movement of the kite and boat, as well as the position of the boat relative to the waypoint, allowing it to gain an idea of direction. It is easy to see how the number of observations could rapidly increase, but this would also increase the complexity of the network and the likelihood of the agent becoming `confused' by the data its receiving. With this in mind the goal is to provide all the required information about the state in the minimum number of observations possible, this is achieved by combining vectors where appropriate. Another consideration when thinking about the observations was that they should be possible to collect if this system were to be created in the real world. This means that the observations should be possible to collect using sensors, such as GPS, wind speed and direction, and accelerometers. The observations used in this project are shown in table$~$\ref{observations}.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c}
        Observation & Vector Size & Description \\
        \midrule
        Waypoint Position & 3 & The position of the waypoint relative to the origin \\
        Distance to the waypoint & 1 & The distance to the waypoint from the boat \\
        Boat Position & 3 & The position of the boat relative to the origin \\
        Boat Speed & 1 & The speed of the boat in the forwards direction \\
        Relative Boat Angle & 1 & The angle of the boat relative to the wind \\
        Kite Position & &\\
        Kite altitude & &\\
        \hline
    \end{tabular}
    \caption{Observations}\label{observations}
\end{table}

The total number of observations passed to the network is .......

\subsection{Actions}
When the agent starts to train it has no idea what to do, it is essentially a blank slate, so starts by randomly flicking around the actions. The agent was given a discrete action space of 6 actions, these are shown in table$~$\ref{actions}. The actions are passed to the network as a vector of 6 values, each value is a float between 0 and 1. The network then interprets these values and outputs the appropriate action. The actions are then passed to the OnActionReceived method, which in calls the methods that control the kite and boat. Discrete actions were chosen because then the rate of change of the angles that control the rudder and kite is not the choice of the network. This gives the network an easier job and means the amount of freedom the network has can be configured. i.e. if the network could pick any value between 0 and 1 for the bar position, it would see far more aggressive controls making it harder for a stable flight to be achieved. On the other had the discrete action space encourages the agent to make a decision and stick with it while the action is being applied. 

These actions were tested as part of making a playable game so that the agent receives the same actions as a human controlling the kiteboat simulation. The Heuristic method that is one of the required functions for the agent script, allows the agent to be controlled by a human. This is useful for testing the environment and the controls, as well as for playing the game. The Heuristic method is called when the agent is not training and so the agent can be controlled by a human. The Heuristic method takes in the action vector and sets the values of the actions to the values of the keyboard inputs. The keyboard inputs are set in the Unity editor and are shown in table$~$\ref{actions}. 

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c}
        Action & States & Description & Keyboard Input\\
        \midrule
        Kite Bar Position & Left, Off, Right & The different states change the bar position & Arrow left/right \\
        Rudder Angle & Left, Off, Right & The angle of the rudder & Keys `A' and `D'\\
        Kite Bar Power & 0, 1 & The power of the kite bar & Space Bar \\
        \hline
    \end{tabular}
    \caption{Actions}\label{actions}
\end{table}


\subsection{Rewards}
Keep alive problem.

Diverse conditions

ciriculum style learning

\section{Initial Training}
Before commencing the training of the kiteboat agent, a simpler test agent was created to check the workflow and ensure the environment was setup correctly. This test agent was a simple cube that was trained to move towards a waypoint, following a pacemaker. This was a good test of the environment as it was a simple task that could be easily visualized as shown in figure .... However this test agent proved more tricky than initially anticipated and after some additional research some core RL training concepts that improve the quality of training were discovered. The first of these being make the problem a `Keep Alive' i.e. do the correct thing or die. In the context of the path follower this meant that should the cube ever be further away from the pacemaker than its original distance of 2m the episode was ended and a large negative reward given. This method encourages the agent to do the correct process if it wants to stay alive. Making the problem a keep alive problem is a simple way to improve the quality of training and is a common practice in RL. The path follower changed from not making much progress over the course of 500000 steps to being able to complete full laps of the course in around 100000 steps, as shown in figure  .....

The next RL technique to help improve training is called Curriculum Learning (CL). CL is an instructional strategy that structures the learning process, much like how a school curriculum guides human learning. It involves organising the leaning tasks from simple to complex, facilitating the agent's ability to incrementally acquire, transfer and refine knowledge. In essence it breaks down large complex tasks into more manageable bitesized chunks that the agent can progress through. CL was not utilised in the path follower but was used in the kiteboat agent. The kiteboat agent was trained in several stages shown below.
\begin{enumerate}
    \item Master the controls- a keep alive problem
    \item Sail Straight downwind
    \item Progressively move the waypoints upwind with each completed waypoint
    \item Randomly generate waypoints in any direction 
\end{enumerate}

Step 1 in the curriculum ensures the agent has a fundamental grasp of the controls and is able to keep the kite in the air. This is a keep alive problem and so the agent is rewarded for keeping the kite in the air and penalised for crashing. The direction the boat sails is not important at this stage. Step 2 is a simple task that the agent can learn quickly, having now gained a grasp of the kite control it must learn to steer straight towards the waypoint. This was also turned into a keep alive problem, move closer to the next waypoint or end the episode. Step 3 is where the agent starts to learn to sail its own path, initially this will still be a straight line until the waypoints are spawning upwind of the kiteboats initial location. At this point the agent must start to experiment with finding the VMG (velocity made good), which is a measure of the speed at which a vessel is moving directly towards its destination, considering both its heading and wind direction. In stage 3 the waypoints will spawn more and more upwind until they are directly upwind, this is in an effort to encourage the agent to find the emergent property of Tacking, a maneuver by which the nose of the boat transitions through the wind while it turns around.  

Figure ... shows the difference in the paths at stages 2 and 3 of the curriculum. The path in stage 2 is a straight line towards the waypoint, while the path in stage 3 is a zigzag as the agent tries to find the VMG. 


Difficulties with initial training approaches:
-The learning of the kite is relativly straight forwards
-Rudder controls were tricky, boat has a habit of hard lock, letting the boat drag sideways seems to be a local maxima for stability,  lets the kite larn bettter
\section{Optimization}

\subsection{Blue Crystal HPC}

The Blue Crystal HPC, operated by the University of Bristol, offers significant computational resources tailored for intensive tasks such as machine learning simulations, like.


To utilize Blue Crystal for MLAgents simulations, the following steps were undertaken:

\begin{itemize}
    \item \textbf{Access and Security:} Gained access to the university's HPC and set up SSH keys for secure communication.
    \item \textbf{File Preparation:} Built the .x86\_64 Unity build file and uploaded it, along with the necessary config file, to Google Drive.
    \item \textbf{Automation with Shell Script:} Developed a shell script to automate the process. This script:
    \begin{itemize}
        \item Retrieves the build files from Google Drive.
        \item Sets up a virtual environment on Blue Crystal.
        \item Installs the ML-Agents toolkit.
        \item Initiates the simulation.
        \item Upon completion, uploads the results back to Google Drive\textsuperscript{3}.
    \end{itemize}
\end{itemize}

Useful commands for working with Blue Crystal:

\begin{itemize}
    \item \textbf{sbatch:} Submits a job to the queue.
    \item \textbf{sacct:} Checks the status of a job.
    \item \textbf{scancel:} Cancels a job.
\end{itemize}

Parallelization and Optimization:

Initially, a single node on Blue Crystal was employed to run the simulation. This node with 28 CPUs was responsible for both hosting the environment and executing the model. However, Blue Crystal's architecture allows for more advanced parallelization strategies. Distributing the simulation across multiple nodes can enhance efficiency. Additionally, offloading the ML-Agents toolkit to a GPU core can further accelerate the learning process.

However, it's worth noting that the demand for GPUs on Blue Crystal is high. For tasks that don't necessitate the power of GPUs, relying on CPUs, even if they take longer, is a practical choice given the limited GPU availability.





%=========================================================